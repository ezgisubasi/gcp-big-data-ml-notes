# Data Engineering for Streaming Data

![image.png](images/Data%20Engineering%20for%20Streaming%20Data/image.png)

Ingest streaming data (Pub/Sub) → Process the data (Dataflow) → Visualize results (Looker)

Batch Processing: when the processing and analysis happens on a set of stored data. 

Ex: Payroll or Billing Systems

Streaming Data: is a flow of data records generated by various data sources. Processing of the streaming data happens as the data flows through a system. This result in the analysis and reporting of events as they happen. 

Ex: Fraud detection

# Big Data Challenges

## 1. Variety

- Data could come in from a **variety (number, image, audio etc.)** of different sources and in various formats.

## 2. Volume

- How do we alert our downstream systems of new transactions in an organized way with no duplicates?

## 3. Velocity

- Handle data that
    - Arrives late
    - Has bad data in the message
    - Needs to be transformed

## 4. Veracity (Data Quality)

- Inconsistency & Uncertainty

# Message-Oriented Architecture

![image.png](images/Data%20Engineering%20for%20Streaming%20Data/image%201.png)

In the data ingestion part of the data pipeline, large amounts of stream data are received. However, data may not always come from a single, structured database. Data might stream from a thousand or even a million different events that are all happening asynchronously. 

Ex: IoT devices, applications

## Pub/Sub (Publisher/Subscriber)

- Handles distributed message-oriented architectures at scale.
- Publish messages to subscribers.
- It is a distributed messaging service that can receive messages from a variety of device streams (Ex: gaming events, application streams)
- Ensures at-least-once delivery of received messages to subscribing applications
- No provisioning is required
- APIs are open
- Global by default
- Offets end-to-end encryption

![image.png](images/Data%20Engineering%20for%20Streaming%20Data/image%202.png)

Upstream source data → Ingest → Pub/Sub: Read, Store, Broadcast (to any subscribers of this data topic that new messages are available) → Dataflow (Subscriber): Ingest, Transform (in a elastic streaming pipeline) → Output results → Analytics data warehouses: BigQuery → Data  visualization & monitoring: Looker → Explore data by AI / ML tools: Vertex AI 

# Designing streaming pipelines with Apache Beam

After messages have been captured from the streaming input sources, you need a way to pipe that data into a data warehouse for analysis.

Input → Ingest → Pipeline(Dataflow)→ Analysis

## Dataflow

![image.png](images/Data%20Engineering%20for%20Streaming%20Data/image%203.png)

- Creates a pipeline to process both streaming data and batch data
- Process: Extract → Transform → Load (ETL)

## Apache Beam : Pipeline Design

- An open source, unified programming model to define and execute date processing pipelines, including ETL, batch and stream processing.
- Unified: means uses single model for both batch and streaming data
- Portable: means it can work on multiple execution environments like Dataflow and Apache Spark
- Extensible: means it allows you to write and share your own connectors and transformation libraries
- Provides pipeline templates
- SDK (Software Development Kit)
- Model representation

# Implementing Streaming Pipelines on Cloud Dataflow

Create Data Processing Pipelines (Apache Beam) → Implement Data Processing Pipelines (Execution Engine)

## Dataflow

- A fully managed service for executing Apache Beam pipelines within the Google Cloud ecosystem
- Handles infrastructure setup
- Handles maintenance
- Built on Google infrastructure

Allows;

- Reliable auto scaling
- Meet data pipeline demands
- Dataflow is serverless and NoOps (No Operations):
    - Maintenance, Monitoring and Scaling ⇒ Automated
    - By this, you can more time analyzing and less time provisioning
- It is designed for low-maintenance
- Apache Beam → Dataflow:
    - Graph optimization
    - Work scheduler
    - Auto-scaler
    - Auto-healing
    - Work rebalancing
    - Compute & Storage → BigQuery

### Dataflow Templates

- Streaming (for processing continuous or real-time data)
    - Pub/Sub to BigQuery
    - Pub/Sub to Cloud Storage
    - Datastream to BigQuery
    - Pub/Sub to MongoFB
- Batch (for processing bulk data or batch load data)
    - BigQuery to Cloud Storage
    - Bigtable to Cloud Storage
    - Cloud Storage to BigQuery
    - Cloud Spanner to Cloud Storage
- Utility (for address activities related to bulk compression, deletion and conversion)
    - Bulk compression of Cloud Stroage files
    - Frestore bulk delete
    - File format conversion

# Visualization with Looker

![image.png](images/Data%20Engineering%20for%20Streaming%20Data/image%204.png)

Pub/Sub → Apache Beam → Dataflow → BigQuery → Looker

- Defines logic and permissions independent from a specific database or SQL language
- Frees a data engineer from interacting with individual databases
- Web-based platform
- You can embed Looker reports to other applications using Looker API (Application Programming Interface)
- Can create Dashboards: Charts, diagrams, funnels etc.